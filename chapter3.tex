 
\let\textcircled=\pgftextcircled
\chapter{Minería de Datos \& \textit{Deep Learning}} {\textit{Este capítulo describe la teoría necesaria sobre   minería de datos y \textit{Deep Learning} para poder entender la propuesta de la investigación}}
\label{cap:deep_learning}



\section{Consideraciones Iniciales}

\initial{L}os rápidos avances en las tecnologías de recolección y de almacenamiento permitieron el almacenamiento de grandes cantidades de datos. Sin embargo, se probó que extraer conocimiento puede ser extremadamente difícil. En este contexto, la minería de datos combina métodos tradicionales de análisis de datos con algoritmos sofisticados para el procesamiento de grandes volúmenes de datos \cite{Tan:2005:IDM:1095618}.

La Minería de Datos (\textit{Data Mining}) es un área de investigación dentro en un contexto más amplio llamado Descubrimiento del Conocimiento en Bases de Datos (\textit{KDD – Knowledge Discovery in Databases}), cuyo objetivo principal es extraer de un conjunto de datos, el conocimiento a ser utilizado en procesos decisorios. Más específicamente, los principales objetivos de los métodos de minería de datos son una descripción de un conjunto de datos y una predicción de valores futuros de interés basado en conocimiento previo de un banco de datos \cite{Fayyad:1996:DMK:257938.257942}.


Como se contrastará   en la siguiente sección,  el enfoque de la minería de datos es  la extracción de patrones, propiedades desconocidas en los datos. En cambio el enfoque del aprendizaje de maquina (\textit{Machine Learning}) es la generación  de modelos de predicción, basado en propiedades conocidas (experiencia) aprendidas de los datos de entrenamiento.   Por otro lado el aprendizaje profundo (\textit{Deep Learning}) es un subconjunto de  del área de Aprendizaje de Maquina. El aprendizaje profundo es un mecanismo de aprendizaje jerárquico (en profundidad) que utiliza un conjunto de algoritmos para diversas tareas de aprendizaje de maquina, mientras que la minería de datos es en su mayoría un proceso de extracción de conocimientos que utiliza varios algoritmos que no necesariamente son ``profundos''. Del mismo modo, el aprendizaje profundo puede ser utilizado para la minería de datos \cite{dlBook, TheDLBook}. 
 
Para hacer frente a  problemas de aprendizaje basado en la experiencia  tenemos  que utilizar un enfoque diferente. Muchas de las cosas que aprendemos en la escuela tienen mucho en común con los programas informáticos tradicionales. Aprendemos cómo multiplicar números, resolver ecuaciones, y hacer derivadas mediante la guía de un conjunto de instrucciones. Pero las cosas que aprendemos a una edad muy temprana, las cosas que nos parecen naturales, se aprenden con el ejemplo, no por fórmulas. Es decir,  en lugar de enseñar a una computadora las reglas para resolver un problema, le damos un modelo con el que se puede evaluar ejemplos y un pequeño conjunto de instrucciones para modificar el modelo cuando se comete un error. Esperando que, con el tiempo, un modelo fuera capaz de resolver el problema con gran precisión \cite{TheDLBook}. 
 
 En la Sección \ref{sec:kdd-datamining} son discutidos los principales conceptos de Minería de Datos y Descubrimiento del Conocimiento en Bases de Datos (KDD). En la Sección \ref{sec:machine-learning}, es  presentado los conceptos de Aprendizaje de Máquina  (\textit{Machine Learning}) y Redes Neuronales. En la Sección \ref{sec:cnn}, se analizan los conceptos de  \acf{CNNs} así como las principales  aplicaciones y casos de éxito. La Sección \ref{sec:autoencoders}, se analiza la teoría de Auto-codificadores y en la Sección \ref{sec:consideraciones-finales} se presentan las consideraciones finales del capítulo.


\section{KDD y Mineria de Datos}\label{sec:kdd-datamining}

\textbf{Descubrimiento de Conocimiento en Base de Datos (KDD)} - es el proceso de: primero, a partir de los datos, identificar patrones válidos, nuevos, potencialmente útiles y comprensivos. Segun  \cite{Fayyad:1996:DMK:257938.257942}, este proceso está compuesto de cinco etapas: selección de los datos; pre-procesamiento y limpieza de los datos; transformación de los datos; Minería de los Datos (\textit{Data Mining}); e interpretación y validación de los resultados. La interpretación entre estas diversas etapas puede ser observada en la Figura \ref{fig:processoKDD}, siendo que las tres primeras pueden ser interpretadas como un análisis exploratorio de los datos.

\begin{figure}[htp]
\centering
\includegraphics[width=0.9\columnwidth]{chapter3/KDD-process.pdf}
\caption{Etapas del proceso KDD. Figura adaptada de \cite{Fayyad:1996:DMK:257938.257942}.}
\label{fig:processoKDD}
\end{figure}


Los principales pasos para el proceso de KDD son:

\begin{enumerate}

    \item \textbf{Limpieza de datos:} Las bases de datos del mundo real a menudo tienen datos incompletos, con ruido e inconsistentes que pueden dañar el análisis dificultando la detección de patrones. Los procedimientos de limpieza de datos trabajan para preparar los datos para los siguientes pasos en el procesos KDD mediante el llenado de los valores que faltan, corrigiendo los datos con ruido, identificando y removiendo valores atípicos, y resolviendo inconsistencia.
    \item \textbf{Integración de datos:} En algunas circunstancias, los datos de múltiples fuentes deben fusionarse y transformadas en formas apropiadas para ser incluidas en el mismo análisis. Por lo tanto, este paso implica técnicas para integrar correctamente múltiples bases de datos, cubos de datos o archivos en un almacenamiento de datos.
    \item \textbf{Selección de datos:} Este paso corresponde a la identificación de datos relevantes para la tarea de análisis y su recuperación a partir de las bases de datos.
    \item \textbf{Transformación de datos:} En este paso, los datos son transformados o consolidados en adecuadas formas para la extracción usando operaciones, tales como el resumen, la agregación, la generalización o la normalización.
    \item \textbf{Minería de datos:} Este paso es fundamental en el proceso KDD, donde se aplican técnicas computacionales para extraer patrones desconocidos y útiles a partir de los datos.
    \item \textbf{Evaluación del patrón:} En este paso, se utilizan medidas interesantes para identificar los patrones que representan el conocimiento.

\end{enumerate}


La etapa de minería de datos incluye la definición de las tareas de minería a ser realizadas, la elección del algoritmo a ser aplicado y la extracción de patrones de interés. En la etapa siguiente, los patrones son interpretados y validados, y si los resultados no fueran satisfactorios (validos, nuevos, útiles y comprensibles), el proceso retorna a uno de los dos estados anteriores. Caso contrario, el conocimiento descubierto es consolidado \cite{Fayyad:1996:DMK:257938.257942}. El proceso KDD se refiere a todo el proceso de descubrimiento de conocimiento útil para los datos, en cuanto a la Minería de los Datos se refiere a la aplicación de algoritmos para extraer modelos de los datos.

 
 
\section{Aprendizaje de Máquina -- \textit{Machine Learning} }\label{sec:machine-learning}

El Aprendizaje de Máquina es un subconjunto de Inteligencia Artificial  que incluye técnicas estadísticas que permiten a las máquinas generar modelos de predicción  basado en propiedades conocidas aprendidas de los datos de entrenamiento, esta categoría incluye el aprendizaje profundo \cite{smola2008ml, TheMLBook}. El Aprendizaje de Máquina se puede clasificar en dos categorías principales. Aprendizaje Supervisado y Aprendizaje no supervisado. 

\subsection{Aprendizaje Supervisado y Redes Neuronales}

El aprendizaje supervisado es una técnica de aprendizaje que utiliza datos de entrenamiento y sobre la observación de estos crea una función capaz de predecir el valor de un nuevo dato y sobre la observación de estos crea una función capaz de predecir el valor de un nuevo dato que la función no haya visto, la supervisión en este tipo de aprendizaje se da mediante instancias etiquetadas en un conjunto de datos de entrenamiento. 

\subsection*{Clasificación}

La clasificación trabaja con conjunto de datos ya participados en grupos, categorías o clases. Este conjunto de datos sirve para entrenar un algoritmo de clasificación. Por ejemplo se tiene una base de datos de correos electrónicos, cada correo tiene un atributo que indica si este es o no un spam, estos datos sirven como datos de entrenamiento para un algoritmo de clasificación, el cual quiere determinar si un nuevo correo es o no spam. 

Para clasificar se pueden usar algoritmos basados en Regresión Logística, en Maquinas de Soporte Vectorial y Redes Neuronales. Nuestro interés son los algoritmos basados en redes neuronales y lo detallamos a continuación.

\subsection*{Redes Neuronales}

Las redes neuronales son modelos que simulan el sistema nervioso humano. La clave para la eficacia de una red neuronal es la arquitectura usada para organizar las conexiones entre nodos. Existe una amplia variedad de arquitecturas, a partir de una red con una sola capa, y otras redes mas complejas con múltiples capas \cite{aggarwal2015data, TheNNBook}.   

 Una arquitectura de red o topología juega un papel importante para la clasificación, la topología óptima dependerá del problema en cuestión. A menudo el conocimiento del dominio del problema que podría ser de carácter informal o heurística puede incorporarse fácilmente en la arquitectura de redes a través de opciones, como número de capas ocultas, unidades, conexiones de retroalimentación. Por lo tanto el establecimiento de la topología de la red es la selección del modelo heurístico. La facilidad práctica en la selección de modelos (topologías de red) y la estimación de parámetros (formación a través de propagación hacia atrás) permiten a los diseñadores de clasificadores probar modelos alternativos de manera bastante simple. \cite{PattClassi, TheNNBook}


En la Figura \ref{fig:perceptron} se puede visualizar la arquitectura de un perceptrón. El perceptrón contiene dos capas de nodos, los cuales corresponden a los nodos de entrada y al nodo de salida, el número de nodos de entrada es exactamente igual a la dimensionalidad d de una instancia, cada nodo de entrada recibe y entrega un atributo numérico al nodo de salida. Los nodos de entrada solo transmiten valores de entrada para el nodo de salida, el nodo de salida es el único que realiza una función matemática en sus entradas.
\begin{figure}[ht]
 \centering
	\includegraphics[width=0.6\columnwidth]{chapter3/perceptron.png}
 \caption{Perceptrón. Figura adaptada de \cite{TheNNBook}.}
 \label{fig:perceptron}
\end{figure}

En el caso de una red neuronal de múltiples capas (Figura \ref{fig:mlp}), se tiene una capa de entrada, una capa de salida, y una o más capas intermedias llamadas capas ocultas. Lo mismo que antes, las salidas multiplicadas por sus pesos, es la entrada de la siguiente capa, sin olvidar que en cada nodo se aplica una función de activación, los nodos de salida son quienes dicen si la instancia pertenece a una clase o no, de acuerdo a la función de activación estos tomarán valores de 0 a 1, o de −1 a +1.
\begin{figure}[ht]
 \centering
	\includegraphics[width=0.75\columnwidth]{chapter3/mlp.png}
 \caption{Perceptrón Multicapa. Figura adaptada de \cite{TheNNBook}.}
 \label{fig:mlp}
\end{figure}

Para entrenar a una red neuronal se realizan dos fases.
 
 \begin{description}
 \item [Fase Forward:] en esta fase se introducen los valores de una instancia en la capa de entrada de la red, los pesos se toman de manera aleatoria, esta fase se realiza hacia adelante, comenzando en la capa de entrada y terminando en la capa de salida, los valore de la capa de salida son comparados con la etiqueta de la instancia para ver si se tiene un error.
 

 \item [Fase Backward:] el objetivo de esta fase es corregir los errores en cada nodo. Recorre la red desde la capa oculta hacia la capa de entrada, en cada nodo se calcula el error y se actualizan los pesos de entrada que recibieron. 
 \end{description}
 
 El proceso de $Forward$ y $Backward$ se repiten para cada instancia, al finalizar se vuelve a repetir todo el proceso para toda la base de datos de entrenamiento, hasta un numero $n$ de épocas si es necesario. 
 
\section{Redes Neuronales Convolutivas (CNN)}\label{sec:cnn}


Una red neuronal convolucional (CNN) es una arquitectura jerárquica \cite{LeCun} que consta de varias capas convolucionales apiladas, opcionalmente seguido por una capa de normalización y una capa de agrupación, capas completamente conectadas y una capa de salida en la parte superior. Usan convoluciones en lugar de multiplicación de matrices en al menos una de sus capas \cite{TheDLBook}.  Las capas convolucionales generan mapas de características por filtros convolucionales lineales seguidos por funciones de activación no lineales (Rectificador, Sigmoide, TanH, etc.). La capa completamente conectada (\textit{fully conected layer}) tiene conexiones completas a todas las activaciones en los mapas de características y el vector unidimensional resultante se puede alimentar en la capa de salida para la optimización de la función de pérdida .

Al igual que el entrenamiento de las redes neuronales tradicionales, en la CNN hay dos etapas principales para entrenar la red neuronal convolucional: una etapa \textit{Forward}  y una etapa de \textit{backward}.  En primer lugar, la etapa de \textit{Forward} es representar la imagen de entrada con los parámetros actuales (pesos y sesgo/bias) en cada capa. A continuación, la salida de la última capa se utiliza para calcular la función de pérdida con las etiquetas de verdad. En segundo lugar, basándose en el costo de la pérdida, la etapa \textit{backward} calcula los gradientes de cada parámetro con reglas de cadena. Todos los parámetros se actualizan en función de los gradientes y se preparan para el siguiente cálculo directo. Después de suficientes iteraciones de las etapas \textit{Forward} y \textit{backward}, la red podría ser optimizada. La red neuronal convolucional ha sido aplicada en diversas aplicaciones de visión por computadora y ha demostrado sus ventajas significativas y alto rendimiento \cite{TheDLBook}.

\subsection {Capas CNN}

En esta seccion presentamos una visión general de los diferentes tipos de capas y luego revisaremos brevemente las aplicaciones de visión computacional basadas en CNN.

\textbf{Capas convolucionales(\textit{Convolucional layers}):} Las capas convolucionales de la arquitectura CNN utilizan $k$ filtros (o núcleos) para envolver la imagen de entrada para generar $k$ mapas de características. Hay tres ventajas principales de la operación de convolución \cite{Zeiler}: en primer lugar, el mecanismo de reparto de parámetros se utiliza en capas convolucionales de tal manera que el número de parámetros podría reducirse significativamente. En segundo lugar, la conectividad local aprende correlaciones entre píxeles vecinos. En tercer lugar, es invariante a la ubicación del objeto. Debido a estos beneficios introducidos por la operación de convolución, algunos trabajos de investigación bien conocidos también lo utilizan como un reemplazo de las capas totalmente conectadas para acelerar el proceso de aprendizaje \cite{Szegedy,Oquab}.

\textbf{Capas de agrupamiento(\textit{Pooling layers}):} Una capa de agrupación es una capa opcional que sigue una capa convolucional que sub-muestrea su entrada. La agrupación media(\textit{average pooling}) y la agrupación máxima(\textit{max pooling}) son las operaciones de agrupación más utilizadas. La razón para utilizar una operación de agrupación en la red neuronal convolucional es que: en primer lugar, puede reducir las dimensiones de la salida y el número de parámetros de la red, manteniendo la información más destacada. En segundo lugar, una operación de agrupación también proporciona una invariancia básica para la traducción (desplazamiento) y la rotación.  Para un \textit{max pooling}g y \textit{average pooling}, Boureau et al. \cite{Boureau} proporcionó un análisis teórico detallado de su funcionamiento. Scherer et al. \cite{Scherer} realizó una comparación entre las dos operaciones de agrupación y encontró que \textit{max pooling} puede conducir a una convergencia más rápida, selección de características invariantes superiores y mejorar la generalización.

\textbf{Capas completamente conectadas(\textit{Fully-connected layers}):}  Después de varias capas convolucionales y de agrupación máxima, el razonamiento de alto nivel en la red neuronal convolucional se realiza a través de las capas completamente conectadas. Una capa completamente conectada toma todas las neuronas de la capa anterior (ya sea totalmente conectado, agrupación o convolucional) y lo conecta a cada neurona que tiene. Las capas completamente conectadas no estan espacialmente localizados , ya que los mapas de características de entrada se convierten en un vector de características unidimensional. El vector de característica unidimensional podría alimentar el \textit{forward} del vector en una capa de pérdida o tomarlo como una representación característica para el procesamiento de seguimiento \cite{Girshick}. El inconveniente de la capa totalmente conectada es que contiene muchos parámetros, lo que da lugar a grandes costes computacionales y de almacenamiento. Por lo tanto, GoogleNet \cite{Szegedy} diseñó una red profunda y amplia, manteniendo el costo computacional constante, cambiando de arquitectura totalmente conectado a escasamente conectadas. La arquitectura \textit{Network in Network} (NIN) \cite{Lin} reemplaza la capa completamente conectada por una capa de agrupación media \textit{average pooling} global.



%%%\subsection{La operación de convolución}
  
 
\section{Aprendizaje no Supervisado}\label{sec:nosupervisado}

En el aprendizaje no supervisado no se tienen instancias etiquetadas, es decir que se tiene un conjunto de datos, en la cual no existe ninguna clase, lo que se trata de hacer es un agrupamiento de estos datos para encontrar o descubrir dichas clases.

\subsection{Auto-codificadores}\label{sec:autoencoders}

    %La flexibilidad de cambio de las redes neuronales es una propiedad muy útil. Esta capacidad de cambio conduce a grandes mejoras en la precisión, en comparación con modelos básicos de aprendizaje de maquina. Una de la primeras mejoras realizadas en el aprendizaje profundo fue el pre-entrenamiento de redes profundas. Este enfoque se basa en la observación de que la inicialización aleatoria es una mala idea, y que pre-entrenar cada capa con un algoritmo de aprendizaje no supervisado puede permitir mejores pesos iniciales \cite{Le15atutorial}.
    Los auto-codificadores pertenecen a una clase de algoritmos de aprendizaje conocidos como aprendizaje no supervisado. A diferencia de los algoritmos supervisadas, los algoritmos de aprendizaje sin supervisión no necesitan información de la etiqueta para los datos. En otras palabras, nuestros datos sólo tienen de $x(i)$, pero no tienen los $y(i)$, que vendrían a ser la etiquetas de los datos \cite{Le15atutorial, website:UFLDL}. Un auto-codificador es una técnica muy utilizada en el aprendizaje no supervisado, aunque también haya sido utilizada de distintas maneras y con distintos objetivos.

    \subsubsection{Compresión de data}

    En \cite{Le15atutorial}, se considera el siguiente ejemplo, se desea desarrollar un programa para enviar algunos datos del teléfono móvil a la nube. Para limitar el uso de la red, se debe optimizar cada \textit{bit} de datos que vamos a enviar. Los datos son una colección de puntos, cada uno tiene dos dimensiones, como se ve en la Figura \ref{fig:aenco1}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.37]{chapter3/aenco1.png}
        \caption{Puntos en un plano 2d. Figura adaptada de \cite{Le15atutorial}.}
        \label{fig:aenco1}
     \end{figure}

    En la Figura \ref{fig:aenco1}, las cruces rojas son los puntos de datos, el eje horizontal es el valor de la primera dimensión y el eje vertical es el valor de la segunda dimensión. Tras la visualización, notamos que el valor de la segunda dimensión es aproximadamente el doble que de la primera dimensión. Teniendo en cuenta esta observación, se puede enviar sólo la primera dimensión de cada punto de datos a la nube. Luego, en la nube, solo se necesita calcular el valor de la segunda dimensión, duplicando el valor de la primera dimensión. La compresión es con perdida, pero reduce el tráfico de red en un 50\%. Ya que el tráfico de red es lo que tratamos de optimizar, esta idea parece razonable \cite{Le15atutorial}.
    \\\\
    El objetivo de los auto-codificadores es poder resolver el ejemplo anterior de manera sistemática. Formalmente, suponemos que tenemos un conjunto de puntos de datos $\{x(1), x(2),$ $..., x(m)\}$, donde cada punto de datos tiene varias dimensiones. La pregunta es si hay una manera general de asignarlos a algún conjuntos de datos $\{z(1), z(2), ..., z(m)\}$, donde $z$ tiene una dimensión menor a $x$ y los $z(i)$ pueden fielmente reconstruir las $x(i)$. Para responder a esto, se nota que en el ejemplo anterior, para enviar datos desde el teléfono celular a la nube, tiene tres pasos:

    \begin{itemize}
        \item Codificación: Desde el celular. se asigna la data $x(i)$ comprimida a $z(i)$.
        \item Envío: Se envía $z(i)$ a la nube.
        \item Decodificación: En la nube, se asigna desde la data comprimida $z(i)$ a $\tilde{x}(i)$, que es una aproximación de $x(i)$.
    \end{itemize}

    Para asignar los datos de un lado a otro de manera sistemática, definimos que $z$ y $\tilde{x}$ son funciones de entrada, de la siguiente manera:

    \begin{equation}
        z{(i)} =W_1x{(i)} + b_1
    \end{equation}
    \begin{equation}
        \tilde{x}{(i)} =W_1z{(i)} + b_1
    \end{equation}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.49]{chapter3/aenco2.png}
        \caption{Arquitectura de auto-codificador no lineal. Figura adaptada de \cite{Le15atutorial}.}
        \label{fig:aenco2}
     \end{figure}
     
    Si $x(i)$ es un vector de dos dimensiones, puede ser posible visualizar los datos para encontrar $W1, W2, b_1, b_2$ analíticamente, donde $W1, W2$ son matrices bidimensionales de pesos y $b_1, b_2$ son el componente \textit{bias}. En casos prácticos, es difícil encontrar esas matrices usando la visualización, por lo que es necesario utilizar el gradiente descendente \cite{stochastic-gradient-tricks}. La meta es tener un $\tilde{x}(i)$ aproximado a $x(i)$, para esto se establece la siguiente función objetivo, que es la suma de diferencia de cuadrados entre $\tilde{x}(i)$ y $x(i)$:
    \begin{equation}
    \begin{aligned}
     J(W_1,b_1,W_2,b_2) & = \sum_{i=1}^m\left(\ \tilde{x}(i)-x(i)\ \right )^{\ 2} \\
      & = \sum_{i=1}^m\left(\ W_2z(i) + b_2-x(i)\ \right )^{\ 2}\\
     & = \sum_{i=1}^m\left(\ W_2 \ (W_1x(i)+b_1) \ + b_2-x(i)\ \right )^{\ 2}
    \end{aligned}
    \end{equation}
    En la Figura \ref{fig:aenco2}, Se observa como se trata de comprimir datos de 4 dimensiones a 2 dimensiones utilizando una red neuronal con una capa oculta. La función de activación de la capa oculta es no lineal. Si los datos fueran altamente no lineales, se podría añadir más capas ocultas a la red para tener un auto-codificador profundo.
    

\section{Consideraciones Finales}\label{sec:consideraciones-finales}

En este capítulo fue presentada una visión general sobre minería de datos y \textit{deep learning}, destacando los conceptos y aplicaciones que son  de interés para este trabajo. En específico, fueron discutidas técnicas de aprendizaje supervisado y no supervisado basado en redes neuronales artificiales con énfasis en arquitecturas CNN y autocodificadores. 

A pesar de las recientes investigaciones en métodos de clasificación,  las redes neuronales basados en arquitecturas CNN  pueden ser la mejor opción en muchos dominios de aplicación debido a su eficiencia en precisión y escalabilidad. En problemas de gran escala con grandes volúmenes de datos las técnicas de aprendizaje profundo,  se muestran más adecuadas conforme el número de ejemplos por clases crece. Además, para tareas de procesamiento y clasificación de los datos  ahora se puede usar el alto rendimiento de las GPUs.    
 



